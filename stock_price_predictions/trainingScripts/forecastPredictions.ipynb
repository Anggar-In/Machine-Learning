{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has been converted into batches via \"split_csv.ipynb\". 1 batch contains 10 stocks ipynb files. \n",
    "# Change the batch folder to do training model and predictions for certain stocks.\n",
    "\n",
    "TRAINING_MODEL_PATH = '../trainingModel'\n",
    "DATASET_PATH = f'../trainingDataset'\n",
    "LOG_DIR = '../trainingLogs'\n",
    "ACCURACY_THRESHOLD = 90\n",
    "MAX_RETRIES = 3\n",
    "WINDOW_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "SPLIT_TIME = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data, (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(data, stats):\n",
    "    stats = np.array(stats)\n",
    "    means = stats[:, 0]\n",
    "    stds = stats[:, 1]\n",
    "    return data * stds + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',', skiprows=1, usecols=(1, 2, 3))\n",
    "    low, high, close = data[:, 0], data[:, 1], data[:, 2]\n",
    "\n",
    "    low_normalized, stats_low = normalize_feature(low)\n",
    "    high_normalized, stats_high = normalize_feature(high)\n",
    "    close_normalized, stats_close = normalize_feature(close)\n",
    "\n",
    "    features = np.stack([low_normalized, high_normalized, close_normalized], axis=1)\n",
    "    times = np.arange(len(data))\n",
    "\n",
    "    return times, features, stats_low, stats_high, stats_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_decimal(value):\n",
    "    \"\"\"Format angka dalam format desimal dengan 2 tempat desimal.\"\"\"\n",
    "    return \"{:.0f}\".format(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_save(models_path, dataset_path, output_path, window_size):\n",
    "    \"\"\"\n",
    "    Load models, forecast 7 days ahead, and save predictions to CSV files.\n",
    "    \"\"\"\n",
    "    for model_file in os.listdir(models_path):\n",
    "        if model_file.endswith('.h5'):\n",
    "            model_path = os.path.join(models_path, model_file)\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            # Extract the corresponding dataset file\n",
    "            dataset_name = model_file.split('_')[0] + '.csv'\n",
    "            dataset_file = os.path.join(dataset_path, dataset_name)\n",
    "\n",
    "            if not os.path.exists(dataset_file):\n",
    "                print(f\"Dataset file for {model_file} not found. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Forecasting for model {model_file} using dataset {dataset_name}...\")\n",
    "\n",
    "            # Load and preprocess the dataset\n",
    "            _, features, stats_low, stats_high, stats_close = parse_data_from_file(dataset_file)\n",
    "            stats = [stats_low, stats_high, stats_close]\n",
    "\n",
    "            # Use the last `window_size` data points for forecasting\n",
    "            input_data = features[-window_size:]\n",
    "            input_data = input_data[np.newaxis, :]  # Add batch dimension\n",
    "\n",
    "            # Forecast for 7 days\n",
    "            forecast = []\n",
    "            for _ in range(15):\n",
    "                pred = model.predict(input_data)\n",
    "                forecast.append(pred.squeeze())\n",
    "                # Append the prediction to the input data for next step\n",
    "                input_data = np.roll(input_data, -1, axis=1)\n",
    "                input_data[0, -1] = pred\n",
    "\n",
    "            # Denormalize the forecast\n",
    "            forecast = np.array(forecast)\n",
    "            forecast_denorm = denormalize_data(forecast, stats)\n",
    "\n",
    "            # Load the original dataset to get the last date\n",
    "            original_data = pd.read_csv(dataset_file)\n",
    "            last_date = pd.to_datetime(original_data['timestamp'].iloc[-1])\n",
    "\n",
    "            # Generate dates for the forecast\n",
    "            forecast_dates = [last_date + timedelta(days=i) for i in range(1, 15)]\n",
    "\n",
    "            # Format the forecast and save to CSV\n",
    "            output_file = os.path.join(output_path, f\"{os.path.splitext(model_file)[0]}_forecast.csv\")\n",
    "            with open(output_file, 'w') as f:\n",
    "                # Write header\n",
    "                f.write('timestamp,low,high,close\\n')\n",
    "                # Write each formatted row\n",
    "                for date, row in zip(forecast_dates, forecast_denorm):\n",
    "                    formatted_row = [date.strftime('%Y-%m-%d')] + [format_decimal(value) for value in row]\n",
    "                    f.write(','.join(formatted_row) + '\\n')\n",
    "\n",
    "            print(f\"Saved forecast to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and call the function\n",
    "OUTPUT_PATH = '../forecastResults'\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "forecast_and_save(TRAINING_MODEL_PATH, DATASET_PATH, OUTPUT_PATH, WINDOW_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAPSTONE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
