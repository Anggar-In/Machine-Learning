{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has been converted into batches via \"split_csv.ipynb\". 1 batch contains 10 stocks ipynb files. \n",
    "# Change the batch folder to do training model and predictions for certain stocks. \n",
    "BATCH_FOLDER = 'batch_26'\n",
    "\n",
    "TRAINING_MODEL_PATH = '../trainingModel'\n",
    "DATASET_PATH = f'../trainingDataset/{BATCH_FOLDER}'\n",
    "LOG_DIR = '../trainingLogs'\n",
    "ACCURACY_THRESHOLD = 90\n",
    "MAX_RETRIES = 3\n",
    "WINDOW_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "SPLIT_TIME = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data, (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(data, stats):\n",
    "    stats = np.array(stats)\n",
    "    means = stats[:, 0]\n",
    "    stds = stats[:, 1]\n",
    "    return data * stds + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, data, window_size):\n",
    "    forecast = []\n",
    "    for time in range(len(data) - window_size):\n",
    "        forecast.append(model.predict(data[time:time + window_size][np.newaxis]))\n",
    "    return np.array(forecast).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',', skiprows=1, usecols=(1, 2, 3))\n",
    "    low, high, close = data[:, 0], data[:, 1], data[:, 2]\n",
    "\n",
    "    low_normalized, stats_low = normalize_feature(low)\n",
    "    high_normalized, stats_high = normalize_feature(high)\n",
    "    close_normalized, stats_close = normalize_feature(close)\n",
    "\n",
    "    features = np.stack([low_normalized, high_normalized, close_normalized], axis=1)\n",
    "    times = np.arange(len(data))\n",
    "\n",
    "    return times, features, stats_low, stats_high, stats_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(time, data, split_time=SPLIT_TIME):\n",
    "    return time[:split_time], data[:split_time], time[split_time:], data[split_time:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(WINDOW_SIZE, 3)),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.1),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.Huber(delta=1.0),\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_values, predicted_values, tolerance=0.05):\n",
    "    differences = np.abs(true_values - predicted_values)\n",
    "    within_tolerance = differences <= (tolerance * true_values)\n",
    "    accuracy = np.mean(within_tolerance) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_series, forecast):\n",
    "    \"\"\"Computes MSE and MAE metrics for the forecast\"\"\"\n",
    "    mse = tf.keras.losses.MSE(true_series, forecast)\n",
    "    mae = tf.keras.losses.MAE(true_series, forecast)\n",
    "    return mse, mae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAPSTONE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
