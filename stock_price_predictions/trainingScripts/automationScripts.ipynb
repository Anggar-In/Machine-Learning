{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has been converted into batches via \"split_csv.ipynb\". 1 batch contains 10 stocks ipynb files. \n",
    "# Change the batch folder to do training model and predictions for certain stocks. \n",
    "BATCH_FOLDER = 'batch_26'\n",
    "\n",
    "TRAINING_MODEL_PATH = '../trainingModel'\n",
    "DATASET_PATH = f'../trainingDataset/{BATCH_FOLDER}'\n",
    "LOG_DIR = '../trainingLogs'\n",
    "ACCURACY_THRESHOLD = 90\n",
    "MAX_RETRIES = 3\n",
    "WINDOW_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "SPLIT_TIME = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data, (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(data, stats):\n",
    "    stats = np.array(stats)\n",
    "    means = stats[:, 0]\n",
    "    stds = stats[:, 1]\n",
    "    return data * stds + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, data, window_size):\n",
    "    forecast = []\n",
    "    for time in range(len(data) - window_size):\n",
    "        forecast.append(model.predict(data[time:time + window_size][np.newaxis]))\n",
    "    return np.array(forecast).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',', skiprows=1, usecols=(1, 2, 3))\n",
    "    low, high, close = data[:, 0], data[:, 1], data[:, 2]\n",
    "\n",
    "    low_normalized, stats_low = normalize_feature(low)\n",
    "    high_normalized, stats_high = normalize_feature(high)\n",
    "    close_normalized, stats_close = normalize_feature(close)\n",
    "\n",
    "    features = np.stack([low_normalized, high_normalized, close_normalized], axis=1)\n",
    "    times = np.arange(len(data))\n",
    "\n",
    "    return times, features, stats_low, stats_high, stats_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(time, data, split_time=SPLIT_TIME):\n",
    "    return time[:split_time], data[:split_time], time[split_time:], data[split_time:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(WINDOW_SIZE, 3)),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.1),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.Huber(delta=1.0),\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_values, predicted_values, tolerance=0.05):\n",
    "    differences = np.abs(true_values - predicted_values)\n",
    "    within_tolerance = differences <= (tolerance * true_values)\n",
    "    accuracy = np.mean(within_tolerance) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_series, forecast):\n",
    "    \"\"\"Computes MSE and MAE metrics for the forecast\"\"\"\n",
    "    mse = tf.keras.losses.MSE(true_series, forecast)\n",
    "    mae = tf.keras.losses.MAE(true_series, forecast)\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename):\n",
    "    print(f\"Processing file: {filename}\")\n",
    "    time, features, stats_low, stats_high, stats_close = parse_data_from_file(filename)\n",
    "\n",
    "    time_train, features_train, time_valid, features_valid = train_val_split(time, features)\n",
    "\n",
    "    train_dataset = windowed_dataset(features_train, WINDOW_SIZE)\n",
    "    validation_dataset = windowed_dataset(features_valid, WINDOW_SIZE)\n",
    "\n",
    "    retries = 0\n",
    "    while retries < MAX_RETRIES:\n",
    "        model = create_model()\n",
    "\n",
    "        # Buat nama subdirektori log berdasarkan nama file dan waktu pelatihan\n",
    "        file_log_dir = os.path.join(\n",
    "            LOG_DIR,\n",
    "            f\"{os.path.basename(filename).split('.')[0]}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        )\n",
    "\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=file_log_dir,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True\n",
    "        )\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            epochs=50,\n",
    "            validation_data=validation_dataset,\n",
    "            callbacks=[early_stopping, reduce_lr, tensorboard_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        rnn_forecast = model_forecast(model, features[SPLIT_TIME - WINDOW_SIZE:], WINDOW_SIZE)\n",
    "        stats = [stats_low, stats_high, stats_close]\n",
    "        rnn_forecast_denorm = denormalize_data(rnn_forecast, stats)\n",
    "        series_valid_denorm = denormalize_data(features_valid, stats)\n",
    "\n",
    "        mse, mae = compute_metrics(series_valid_denorm, rnn_forecast_denorm)\n",
    "        print(f\"mse: {mse.numpy().mean():.2f}, mae: {mae.numpy().mean():.2f} for forecast\")\n",
    "\n",
    "        accuracy = calculate_accuracy(series_valid_denorm, rnn_forecast_denorm, tolerance=0.05)\n",
    "\n",
    "        if accuracy < 70.0:\n",
    "            print(f\"Accuracy ({accuracy:.2f}%) is below 70%. Skipping to the next file.\")\n",
    "            break\n",
    "\n",
    "        if accuracy >= ACCURACY_THRESHOLD:\n",
    "            model_filename = f\"{os.path.basename(filename).split('.')[0]}_A{int(accuracy)}_M{int(mae.numpy().mean())}.h5\"\n",
    "            model_save_path = os.path.join(TRAINING_MODEL_PATH, model_filename)\n",
    "            model.save(model_save_path)\n",
    "            print(f\"Model saved to {model_save_path} with accuracy {accuracy:.2f}% and MAE {mae.numpy().mean():.2f}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Attempt {retries + 1}: Accuracy ({accuracy:.2f}%) below threshold ({ACCURACY_THRESHOLD}%). Retrying...\")\n",
    "            retries += 1\n",
    "\n",
    "    if retries == MAX_RETRIES:\n",
    "        print(f\"Max retries reached for {filename}. Skipping to next file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(DATASET_PATH):\n",
    "    if file.endswith('.csv'):\n",
    "        filepath = os.path.join(DATASET_PATH, file)\n",
    "        process_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_decimal(value):\n",
    "    \"\"\"Format angka dalam format desimal dengan 2 tempat desimal.\"\"\"\n",
    "    return \"{:.0f}\".format(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_save(models_path, dataset_path, output_path, window_size):\n",
    "    \"\"\"\n",
    "    Load models, forecast 7 days ahead, and save predictions to CSV files.\n",
    "    \"\"\"\n",
    "    for model_file in os.listdir(models_path):\n",
    "        if model_file.endswith('.h5'):\n",
    "            model_path = os.path.join(models_path, model_file)\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            # Extract the corresponding dataset file\n",
    "            dataset_name = model_file.split('_')[0] + '.csv'\n",
    "            dataset_file = os.path.join(dataset_path, dataset_name)\n",
    "\n",
    "            if not os.path.exists(dataset_file):\n",
    "                print(f\"Dataset file for {model_file} not found. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Forecasting for model {model_file} using dataset {dataset_name}...\")\n",
    "\n",
    "            # Load and preprocess the dataset\n",
    "            _, features, stats_low, stats_high, stats_close = parse_data_from_file(dataset_file)\n",
    "            stats = [stats_low, stats_high, stats_close]\n",
    "\n",
    "            # Use the last `window_size` data points for forecasting\n",
    "            input_data = features[-window_size:]\n",
    "            input_data = input_data[np.newaxis, :]  # Add batch dimension\n",
    "\n",
    "            # Forecast for 7 days\n",
    "            forecast = []\n",
    "            for _ in range(7):\n",
    "                pred = model.predict(input_data)\n",
    "                forecast.append(pred.squeeze())\n",
    "                # Append the prediction to the input data for next step\n",
    "                input_data = np.roll(input_data, -1, axis=1)\n",
    "                input_data[0, -1] = pred\n",
    "\n",
    "            # Denormalize the forecast\n",
    "            forecast = np.array(forecast)\n",
    "            forecast_denorm = denormalize_data(forecast, stats)\n",
    "\n",
    "            # Load the original dataset to get the last date\n",
    "            original_data = pd.read_csv(dataset_file)\n",
    "            last_date = pd.to_datetime(original_data['timestamp'].iloc[-1])\n",
    "\n",
    "            # Generate dates for the forecast\n",
    "            forecast_dates = [last_date + timedelta(days=i) for i in range(1, 8)]\n",
    "\n",
    "            # Format the forecast and save to CSV\n",
    "            output_file = os.path.join(output_path, f\"{os.path.splitext(model_file)[0]}_forecast.csv\")\n",
    "            with open(output_file, 'w') as f:\n",
    "                # Write header\n",
    "                f.write('timestamp,low,high,close\\n')\n",
    "                # Write each formatted row\n",
    "                for date, row in zip(forecast_dates, forecast_denorm):\n",
    "                    formatted_row = [date.strftime('%Y-%m-%d')] + [format_decimal(value) for value in row]\n",
    "                    f.write(','.join(formatted_row) + '\\n')\n",
    "\n",
    "            print(f\"Saved forecast to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAPSTONE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
